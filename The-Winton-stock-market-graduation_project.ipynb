{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "test_data = pd.read_csv('test_2.csv')\n",
    "test_data = pd.DataFrame(test_data)\n",
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Ret_111</th>\n",
       "      <th>Ret_112</th>\n",
       "      <th>Ret_113</th>\n",
       "      <th>Ret_114</th>\n",
       "      <th>Ret_115</th>\n",
       "      <th>Ret_116</th>\n",
       "      <th>Ret_117</th>\n",
       "      <th>Ret_118</th>\n",
       "      <th>Ret_119</th>\n",
       "      <th>Ret_120</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.412783</td>\n",
       "      <td>-0.056284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.413226</td>\n",
       "      <td>18871</td>\n",
       "      <td>0.2138</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>-0.000762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>-0.002444</td>\n",
       "      <td>-0.001301</td>\n",
       "      <td>-0.000917</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>-0.000556</td>\n",
       "      <td>0.000759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.907973</td>\n",
       "      <td>1.002425</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.257825</td>\n",
       "      <td>5852</td>\n",
       "      <td>0.2138</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000463</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>-0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.607583</td>\n",
       "      <td>1.076668</td>\n",
       "      <td>0.517865</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.947340</td>\n",
       "      <td>76935</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>-0.000181</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.230240</td>\n",
       "      <td>0.223222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84573</td>\n",
       "      <td>0.3318</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000792</td>\n",
       "      <td>-0.000479</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>-0.001079</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>-0.001850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.360399</td>\n",
       "      <td>0.597896</td>\n",
       "      <td>-0.145497</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.275744</td>\n",
       "      <td>89615</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.000233</td>\n",
       "      <td>-0.000495</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000377</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>-0.000114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0   1        1.0   1.412783  -0.056284        NaN       10.0   0.413226   \n",
       "1   2        NaN        NaN   0.907973   1.002425        7.0  -0.257825   \n",
       "2   3        NaN  -0.607583   1.076668   0.517865        5.0   0.947340   \n",
       "3   4        NaN   2.230240   0.223222        NaN        1.0        NaN   \n",
       "4   5        NaN   0.360399   0.597896  -0.145497       10.0   0.275744   \n",
       "\n",
       "   Feature_7  Feature_8  Feature_9  ...   Ret_111   Ret_112   Ret_113  \\\n",
       "0      18871     0.2138       11.0  ...  0.000370 -0.000762       NaN   \n",
       "1       5852     0.2138       13.0  ...  0.000457  0.000003 -0.000007   \n",
       "2      76935     0.0105       10.0  ...  0.000003  0.000157 -0.000181   \n",
       "3      84573     0.3318       13.0  ...  0.000010 -0.000792 -0.000479   \n",
       "4      89615     0.0099        8.0  ...  0.000447  0.000489 -0.000233   \n",
       "\n",
       "    Ret_114   Ret_115   Ret_116   Ret_117   Ret_118   Ret_119   Ret_120  \n",
       "0  0.000366 -0.002444 -0.001301 -0.000917  0.000762 -0.000556  0.000759  \n",
       "1 -0.000003 -0.000012 -0.000463 -0.000003 -0.000002  0.000468 -0.000012  \n",
       "2  0.000003       NaN  0.000164  0.000353  0.000704 -0.000168 -0.000006  \n",
       "3  0.000017  0.000170 -0.001079  0.000320  0.000006  0.001392 -0.001850  \n",
       "4 -0.000495  0.000057 -0.000050 -0.000011 -0.000377  0.000227 -0.000114  \n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "train_data = pd.DataFrame(train_data)\n",
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Ret_175</th>\n",
       "      <th>Ret_176</th>\n",
       "      <th>Ret_177</th>\n",
       "      <th>Ret_178</th>\n",
       "      <th>Ret_179</th>\n",
       "      <th>Ret_180</th>\n",
       "      <th>Ret_PlusOne</th>\n",
       "      <th>Ret_PlusTwo</th>\n",
       "      <th>Weight_Intraday</th>\n",
       "      <th>Weight_Daily</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75751</td>\n",
       "      <td>0.2254</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002688</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>-0.000838</td>\n",
       "      <td>-6.953224e-04</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.001974</td>\n",
       "      <td>-0.019512</td>\n",
       "      <td>0.028846</td>\n",
       "      <td>1.251508e+06</td>\n",
       "      <td>1.564385e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.388896</td>\n",
       "      <td>17369</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>3.315418e-07</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.002939</td>\n",
       "      <td>-0.010253</td>\n",
       "      <td>1.733950e+06</td>\n",
       "      <td>2.167438e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.696727</td>\n",
       "      <td>0.739591</td>\n",
       "      <td>-0.167928</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.471947</td>\n",
       "      <td>8277</td>\n",
       "      <td>0.3650</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000524</td>\n",
       "      <td>-0.000394</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>5.322557e-04</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>-0.024791</td>\n",
       "      <td>0.015711</td>\n",
       "      <td>1.529197e+06</td>\n",
       "      <td>1.911497e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.694350</td>\n",
       "      <td>1.568248</td>\n",
       "      <td>0.479073</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.120653</td>\n",
       "      <td>22508</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>-1.281102e-04</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>-0.005680</td>\n",
       "      <td>-0.002190</td>\n",
       "      <td>1.711569e+06</td>\n",
       "      <td>2.139462e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.736489</td>\n",
       "      <td>2.765531</td>\n",
       "      <td>1.245280</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.866985</td>\n",
       "      <td>22423</td>\n",
       "      <td>0.2138</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001235</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>8.619882e-06</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.036104</td>\n",
       "      <td>-0.026552</td>\n",
       "      <td>1.267270e+06</td>\n",
       "      <td>1.584088e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0   1        NaN        NaN        NaN        NaN        8.0        NaN   \n",
       "1   2        NaN        NaN        NaN        NaN        3.0   0.388896   \n",
       "2   3        NaN  -0.696727   0.739591  -0.167928        9.0   0.471947   \n",
       "3   4        NaN  -0.694350   1.568248   0.479073        5.0   0.120653   \n",
       "4   5        6.0  -1.736489   2.765531   1.245280        7.0   4.866985   \n",
       "\n",
       "   Feature_7  Feature_8  Feature_9  ...   Ret_175   Ret_176   Ret_177  \\\n",
       "0      75751     0.2254       11.0  ... -0.002688  0.002246 -0.000838   \n",
       "1      17369     0.0166       13.0  ... -0.000129  0.000123  0.000248   \n",
       "2       8277     0.3650        9.0  ... -0.000524 -0.000394  0.000116   \n",
       "3      22508     0.2654       13.0  ...  0.000346 -0.000090  0.000288   \n",
       "4      22423     0.2138       13.0  ... -0.001235  0.000027  0.002449   \n",
       "\n",
       "        Ret_178   Ret_179   Ret_180  Ret_PlusOne  Ret_PlusTwo  \\\n",
       "0 -6.953224e-04  0.000003 -0.001974    -0.019512     0.028846   \n",
       "1  3.315418e-07  0.000003  0.000027    -0.002939    -0.010253   \n",
       "2  5.322557e-04  0.000274  0.000784    -0.024791     0.015711   \n",
       "3 -1.281102e-04  0.000074  0.000341    -0.005680    -0.002190   \n",
       "4  8.619882e-06  0.001209 -0.000004     0.036104    -0.026552   \n",
       "\n",
       "   Weight_Intraday  Weight_Daily  \n",
       "0     1.251508e+06  1.564385e+06  \n",
       "1     1.733950e+06  2.167438e+06  \n",
       "2     1.529197e+06  1.911497e+06  \n",
       "3     1.711569e+06  2.139462e+06  \n",
       "4     1.267270e+06  1.584088e+06  \n",
       "\n",
       "[5 rows x 211 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "Shape of training feature data: (40000, 30)\n",
      "Shape of training target data: (40000, 2)\n",
      "Shape of test feature data: (120000, 30)\n"
     ]
    }
   ],
   "source": [
    "# Define visualization mode\n",
    "ANALYSE_DATA = True\n",
    "\n",
    "# Aggregate intraday returns\n",
    "intraday_rets = []\n",
    "rets = ['Ret_MinusTwo', 'Ret_MinusOne']\n",
    "train_aggregated_rets = pd.DataFrame(columns=['Ret_Agg', 'Ret_Agg_Std', 'Ret_Std', ])\n",
    "test_aggregated_rets = pd.DataFrame(columns=['Ret_Agg', 'Ret_Agg_Std', 'Ret_Std'])\n",
    "\n",
    "for i in range(2, 121):\n",
    "    intraday_rets.append(f'Ret_{i}')\n",
    "\n",
    "train_aggregated_rets['Ret_Agg'] = train_data[intraday_rets].sum(axis=1)\n",
    "train_aggregated_rets['Ret_Agg_Std'] = train_data[intraday_rets].std(axis=1)\n",
    "train_aggregated_rets['Ret_Std'] = train_data[rets].std(axis=1)\n",
    "train_data = pd.concat([train_data, train_aggregated_rets], axis=1)\n",
    "\n",
    "test_aggregated_rets['Ret_Agg'] = test_data[intraday_rets].sum(axis=1)\n",
    "test_aggregated_rets['Ret_Agg_Std'] = test_data[intraday_rets].std(axis=1)\n",
    "test_aggregated_rets['Ret_Std'] = test_data[rets].std(axis=1)\n",
    "test_data = pd.concat([test_data, test_aggregated_rets], axis=1)\n",
    "\n",
    "# Prepare train, validation and test data\n",
    "features = ['Feature_1', 'Feature_2', 'Feature_3', 'Feature_4', 'Feature_5', 'Feature_6',\n",
    "            'Feature_7', 'Feature_8', 'Feature_9', 'Feature_10', 'Feature_11', 'Feature_12',\n",
    "            'Feature_13', 'Feature_14', 'Feature_15', 'Feature_16', 'Feature_17', 'Feature_18',\n",
    "            'Feature_19', 'Feature_20', 'Feature_21', 'Feature_22', 'Feature_23', 'Feature_24',\n",
    "            'Feature_25', 'Ret_MinusTwo', 'Ret_MinusOne', 'Ret_Agg', 'Ret_Agg_Std', 'Ret_Std', ]\n",
    "targets = ['Ret_PlusOne', 'Ret_PlusTwo']\n",
    "weights_intraday = 'Weight_Intraday'\n",
    "weights_daily = 'Weight_Daily'\n",
    "weights = [weights_intraday, weights_daily]\n",
    "features_targets = features + targets\n",
    "\n",
    "\n",
    "train_X_Y_df = train_data[features + targets]\n",
    "train_X_df = train_data[features]\n",
    "train_Y_df = train_data[targets]\n",
    "train_weights_daily_df = train_data[weights_daily]\n",
    "test_X_df = test_data[features]\n",
    "\n",
    "\n",
    "print('Data loaded')\n",
    "print(f'Shape of training feature data: {train_X_df.shape}')\n",
    "print(f'Shape of training target data: {train_Y_df.shape}')\n",
    "print(f'Shape of test feature data: {test_X_df.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Analysis Constrains** <a class=\"anchor\" id=\"analysis_constrains\"></a>\n",
    "\n",
    " In the interest of time we will focus only on predicting Ret_PlusOne and Ret_PlusTwo. To further justify this, we also note\n",
    " that intraday one min data is **_likely_** to be too noisy to be predictied by the data given in this competition. Hence we will only\n",
    " output predictions for {id}_61 and {id}_62 and output 0 for all of {id}_{1-60}.\n",
    " \n",
    " ## ** Data Analysis** <a class=\"anchor\" id=\"data_analysis\"></a>\n",
    "\n",
    " Here we will do initial data analysis. For each feature in the data set we look at the number of\n",
    " missing values, how many values are unique, how imbalanced the values are, how many potential\n",
    " outliers are contained in the values and if values in the training and test sets are disjoint.\n",
    "\n",
    " We define potential outliers as values which differs from the mean by more than 3 standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing data (this will take a while)...\n",
      "Data Analysis:\n",
      "               0  Missing  Missing %  Unique  Unique %  Imbalance  \\\n",
      "0      Feature_1    33313    83.2825      11    0.0275       2651   \n",
      "1      Feature_2     9146    22.8650   30855   77.1375          1   \n",
      "2      Feature_3     1237     3.0925   38764   96.9100          1   \n",
      "3      Feature_4     7721    19.3025   32280   80.7000          1   \n",
      "4      Feature_5        0     0.0000      10    0.0250       6943   \n",
      "5      Feature_6     1933     4.8325   38068   95.1700          1   \n",
      "6      Feature_7        0     0.0000     824    2.0600        114   \n",
      "7      Feature_8      469     1.1725      33    0.0825       4178   \n",
      "8      Feature_9     1875     4.6875      37    0.0925       5863   \n",
      "9     Feature_10    19471    48.6775       7    0.0175      14437   \n",
      "10    Feature_11      987     2.4675   39014   97.5350          1   \n",
      "11    Feature_12     1096     2.7400     102    0.2550       1596   \n",
      "12    Feature_13      594     1.4850      11    0.0275       4715   \n",
      "13    Feature_14      728     1.8200   39273   98.1825          1   \n",
      "14    Feature_15     2141     5.3525     921    2.3025         61   \n",
      "15    Feature_16      610     1.5250       3    0.0075      39100   \n",
      "16    Feature_17      646     1.6150   39355   98.3875          1   \n",
      "17    Feature_18      568     1.4200   39433   98.5825          1   \n",
      "18    Feature_19     1190     2.9750   38811   97.0275          1   \n",
      "19    Feature_20     7826    19.5650      10    0.0250       7008   \n",
      "20    Feature_21     1018     2.5450   38983   97.4575          1   \n",
      "21    Feature_22     1345     3.3625   38656   96.6400          1   \n",
      "22    Feature_23     1711     4.2775   38290   95.7250          1   \n",
      "23    Feature_24      726     1.8150   39275   98.1875          1   \n",
      "24    Feature_25      655     1.6375   39346   98.3650          1   \n",
      "25  Ret_MinusTwo        0     0.0000   40000  100.0000          1   \n",
      "26  Ret_MinusOne        0     0.0000   40000  100.0000          1   \n",
      "27       Ret_Agg        0     0.0000   40000  100.0000          1   \n",
      "28   Ret_Agg_Std        0     0.0000   40000  100.0000          1   \n",
      "29       Ret_Std        0     0.0000   40000  100.0000          1   \n",
      "30   Ret_PlusOne        0     0.0000   40000  100.0000          1   \n",
      "31   Ret_PlusTwo        0     0.0000   40000  100.0000          1   \n",
      "\n",
      "    Imbalance %  Outlier  Outlier % Disjoint  \n",
      "0        6.6275        0     0.0000    False  \n",
      "1        0.0025       62     0.1550     True  \n",
      "2        0.0025      269     0.6725     True  \n",
      "3        0.0025       80     0.2000     True  \n",
      "4       17.3575        0     0.0000    False  \n",
      "5        0.0025     1018     2.5450     True  \n",
      "6        0.2850        0     0.0000     True  \n",
      "7       10.4450        0     0.0000    False  \n",
      "8       14.6575      237     0.5925    False  \n",
      "9       36.0925      778     1.9450    False  \n",
      "10       0.0025      605     1.5125     True  \n",
      "11       3.9900        0     0.0000    False  \n",
      "12      11.7875        0     0.0000    False  \n",
      "13       0.0025      552     1.3800     True  \n",
      "14       0.1525      888     2.2200    False  \n",
      "15      97.7500      290     0.7250    False  \n",
      "16       0.0025      486     1.2150     True  \n",
      "17       0.0025      894     2.2350     True  \n",
      "18       0.0025       15     0.0375     True  \n",
      "19      17.5200        0     0.0000    False  \n",
      "20       0.0025      950     2.3750     True  \n",
      "21       0.0025      118     0.2950     True  \n",
      "22       0.0025     1434     3.5850     True  \n",
      "23       0.0025      798     1.9950     True  \n",
      "24       0.0025     1527     3.8175     True  \n",
      "25       0.0025      635     1.5875     True  \n",
      "26       0.0025      689     1.7225    False  \n",
      "27       0.0025      673     1.6825     True  \n",
      "28       0.0025      611     1.5275     True  \n",
      "29       0.0025      684     1.7100     True  \n",
      "30       0.0025      625     1.5625      NaN  \n",
      "31       0.0025      655     1.6375      NaN  \n"
     ]
    }
   ],
   "source": [
    "def outliers(col):\n",
    "    std3 = col.std() * 3\n",
    "    mean = col.mean()\n",
    "    c = 0\n",
    "    for row in col:\n",
    "        if (abs(row - mean) > std3):\n",
    "            c = c + 1\n",
    "    return c\n",
    "\n",
    "def analyse_df(name, df_train, df_test=None, percentage=True):\n",
    "    test_set = ()\n",
    "    vals = []\n",
    "    vals_percent = []\n",
    "    for col in df_train:\n",
    "        if df_test is not None:\n",
    "            test_set = set(df_test[col])\n",
    "        switcher = {\n",
    "            'Missing': sum(df_train[col].isnull()),\n",
    "            'Unique': len(df_train[col].unique()),\n",
    "            'Imbalance': df_train[col].value_counts().values[0],\n",
    "            'Outlier': outliers(df_train[col]),\n",
    "            'Disjoint': set(df_train[col]).isdisjoint(test_set)\n",
    "        }\n",
    "        val = switcher.get(name)\n",
    "        vals.append(val)\n",
    "        vals_percent.append(val/len(df_train[col])*100)\n",
    "    if percentage:\n",
    "        res_df = pd.DataFrame(list(zip(vals, vals_percent)), columns=[name, f'{name} %'])\n",
    "    else:\n",
    "        res_df = pd.DataFrame(list(zip(vals)), columns=[name])\n",
    "    return res_df\n",
    "\n",
    "\n",
    "# if ANALYSE_DATA:\n",
    "# OBS: This is very compute intensive\n",
    "print('Analysing data (this will take a while)...')\n",
    "missing_data = analyse_df('Missing', train_X_Y_df)\n",
    "unique_data = analyse_df('Unique', train_X_Y_df)\n",
    "balanced_data = analyse_df('Imbalance', train_X_Y_df)\n",
    "outlier_data = analyse_df('Outlier', train_X_Y_df)\n",
    "disjoint_data = analyse_df('Disjoint', train_X_df, test_X_df, False)\n",
    "analyse_data = pd.concat([pd.DataFrame(train_X_Y_df.columns), missing_data, unique_data,\n",
    "                          balanced_data, outlier_data, disjoint_data], axis=1)\n",
    "print(\"Data Analysis:\")\n",
    "print(analyse_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From this analysis we observe the following:\n",
    "\n",
    " - Feature_1 has a very high number of missing values. It is likely that dropping this feature will improve model performance.\n",
    "\n",
    " - Feature_1, Feature_5, Feature_7, Feature_8, Feature_9, Feature_12, Feature_13, Feature_16 and Feature_20 contains few\n",
    "   uniqe values. These features are therefore determined to be categorical.\n",
    "\n",
    " - Feature_15 is highly imbalanced.\n",
    "\n",
    " - Besides being categorical, Feature_6 and Feature_7 are also distinct between the training and the test data sets.\n",
    "   However, Feature_6 is highly unique, whereas Feature_7 only contains relatively few unique values.\n",
    "\n",
    " This leaves Feature_7 as a special feature. We will try investigate this feature more thoroughly, by grouping returns by values of Feature_7 and\n",
    " look for a relationship in the sign of the returns to see if equal values of Feature_7 has correlated returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequence of return signs: 0.9886483932059086\n"
     ]
    }
   ],
   "source": [
    "if ANALYSE_DATA:\n",
    "    feature_groups = defaultdict(list)\n",
    "    for i, row in train_data.iterrows():\n",
    "        val = row['Ret_PlusOne']\n",
    "        if val > 0:\n",
    "            feature_groups[row['Feature_7']].append(1)\n",
    "        else:\n",
    "            feature_groups[row['Feature_7']].append(-1)\n",
    "\n",
    "    freq = 0\n",
    "    for key, val in feature_groups.items():\n",
    "        frq0 = val.count(1)/len(val)\n",
    "        frq1 = len(val) - frq0\n",
    "        frq = max(frq0, frq1)\n",
    "        freq += frq/len(val)\n",
    "    freq = freq / len(feature_groups)\n",
    "\n",
    "    print(f'Frequence of return signs: {freq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We observe that the sign of the returns within each group is almost 100% correlated. Regardless of what the nature of Feature_7 is,\n",
    " we have to account for this relationship when doing model cross-validation to avoid data leakage. Most likely, Feature_7\n",
    " has a relationship to time, which would explain why returns within the groups are correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ** Preprocessing** <a class=\"anchor\" id=\"preprocessing\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Feature Selection** <a class=\"anchor\" id=\"feature_selection\"></a>\n",
    "\n",
    " We will be using a robust preprocessing and modelling approach - hence, we include all features. Based on the above analysis, we split the features into\n",
    " numerical and categorical. We further split categorical into ordered (Feature_13) and unordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define final features\n",
    "num_features_final = ['Feature_2', 'Feature_3', 'Feature_4', 'Feature_6',\n",
    "                      'Feature_11', 'Feature_14',\n",
    "                      'Feature_17', 'Feature_18', 'Feature_19',\n",
    "                      'Feature_21', 'Feature_22', 'Feature_23', 'Feature_24', 'Feature_25',\n",
    "                      'Ret_MinusTwo', 'Ret_MinusOne', 'Ret_Agg', 'Ret_Agg_Std',\n",
    "                      'Ret_Std']\n",
    "\n",
    "cat_features_ordinal_final = ['Feature_13']\n",
    "\n",
    "cat_features_nominal_final = ['Feature_1', 'Feature_5', 'Feature_7', 'Feature_8', 'Feature_9', 'Feature_10',\n",
    "                              'Feature_12', 'Feature_15', 'Feature_16', 'Feature_20']\n",
    "\n",
    "cat_features_final = cat_features_ordinal_final + cat_features_nominal_final\n",
    "features_final = num_features_final + cat_features_final\n",
    "\n",
    "train_X_df = train_data[features_final]\n",
    "test_X_df = test_data[features_final]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Data Preprocessing** <a class=\"anchor\" id=\"data_preprocessing\"></a>\n",
    "\n",
    " Here we define the piplines for preprocessing the input features and targets.\n",
    "\n",
    " We will impute both numerical and categorical features using a constant. This is chosen as the most conservative option.\n",
    "\n",
    " For numerical features, we will scale them using quantile range = [5, 95] as this approach is robust to outliers. Then we\n",
    " try to forces the distribution to be gaussian by using a quantile transformation. This is justified by the tendency of\n",
    " linear estimators to perform better on gaussian distributions. We also cut off values which lies\n",
    " more than 3 standard deviations from the centre to improve our handling of outliers.\n",
    "\n",
    " For ordered categorical features, we encode them using OrdinalEncoder to preserve the ordering.\n",
    "\n",
    " For unordered categorical features, we remove correlations between features by using PCA whitening and then one-hot encode so we can\n",
    " use models in the model building step that does not deal natively with categorical features.\n",
    "\n",
    " Finally, we normalize all features using l2-norm. The last step seems to improve l2-regulized regression significantly.\n",
    "\n",
    " *NOTE: We would have expected that PCA whitening of the numerical features would be beneficial for model performance since\n",
    " many of the numerical features are highly correlated. However, the opposite seems to be true. Perhaps the correlation clusters\n",
    " apparent in the correlation heatmap contains interesting information. We will leave it to a future improvement to try to\n",
    " investigate this further.*\n",
    "\n",
    " For target returns, we also use a quantile transformation to force the distribution of returns (which are not normally gaussian)\n",
    " into a gaussian distribution. We could also have used log transformation, but quantile transformation yields a better\n",
    " model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessor_X output shape: (40000, 1020)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "class CutOff(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        X[X > 3] = 3\n",
    "        X[X < -3] = -3\n",
    "        return X\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant')),\n",
    "    ('scale', RobustScaler(quantile_range=[5, 95])),\n",
    "    ('quantile', QuantileTransformer(n_quantiles=300, output_distribution='normal', random_state=0)),\n",
    "    ('cutoff', CutOff()),  # Cut off at 3 standard deviations\n",
    "    ('norm', Normalizer(norm='l2'))\n",
    "])\n",
    "\n",
    "# Preprocessing for nominal categorical data\n",
    "cat_transformer_nominal = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant')),\n",
    "    ('pca', PCA(whiten=True, random_state=0)),\n",
    "    ('bins', KBinsDiscretizer(n_bins=100, encode='onehot', strategy='quantile')),\n",
    "    ('norm', Normalizer(norm='l2')),\n",
    "])\n",
    "\n",
    "# Preprocessing for ordinal categorical data\n",
    "cat_transformer_ordinal = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant')),\n",
    "    ('bins', KBinsDiscretizer(n_bins=100, encode='ordinal', strategy='quantile')),\n",
    "    ('norm', Normalizer(norm='l2')),\n",
    "])\n",
    "\n",
    "# Combined preprocessing for numerical and categorical data\n",
    "preprocessor_X = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_features_final),        \n",
    "        ('cat_nom', cat_transformer_nominal, cat_features_nominal_final),\n",
    "        ('cat_ord', cat_transformer_ordinal, cat_features_ordinal_final)        \n",
    "    ])\n",
    "\n",
    "# Testing preprocessor\n",
    "preprocessor_X_shape = preprocessor_X.fit_transform(train_X_df).shape\n",
    "print(f'preprocessor_X output shape: {preprocessor_X_shape}')\n",
    "\n",
    "# Target transformer\n",
    "preprocessor_Y = Pipeline(steps=[\n",
    "    ('quantile', QuantileTransformer(n_quantiles=300, output_distribution='normal', random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ** Modelling** <a class=\"anchor\" id=\"modelling\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Model Building** <a class=\"anchor\" id=\"model_building\"></a>\n",
    "\n",
    " We will focus on LinearSVR regression (wrapper for liblinear) with l2 regularization, as this method seems to deals particullary well\n",
    " with data with a very low Signal-to-noise ratio as one would expect from financial data. It is also a very fast algorithm (liblinear is heavily optimized).\n",
    " We will do a grid search with 5-fold GroupKFold cross-validation. As mentioned earlier, the fact that returns are not independent of Feature_7, we will have\n",
    " to group our cross-validation in order to avoid data leakage and hence overestimation of the CV performance*.\n",
    "\n",
    " Ideally, we should optimize using a loss function suitable for optimizing Weighed Mean Absolute Error (which is non-differentiable at 0). We did not\n",
    " prioritize this, and we still got reasonable results in the model scoring.\n",
    "\n",
    " (*)See: https://stats.stackexchange.com/questions/95797/how-to-split-the-dataset-for-cross-validation-learning-curve-and-final-evaluat\n",
    " and http://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Running grid search CV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\yahya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters = {'regressor__multioutreg__estimator__C': 0.0005}\n",
      "Best MAE = 0.015531589538141878\n",
      "Done building model\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "GRIDSEARCH=True\n",
    "print('Building model...')\n",
    "\n",
    "# Define initial model\n",
    "model = LinearSVR(epsilon=0.0, C=0.0005, loss='squared_epsilon_insensitive', random_state=0)  # 1727.860\n",
    "\n",
    "# Define model pipeline for multi output regression\n",
    "multi_out_reg = MultiOutputRegressor(model)\n",
    "model_pipeline = Pipeline(steps=[('preprocessor', preprocessor_X), ('multioutreg', multi_out_reg)])\n",
    "estimator = TransformedTargetRegressor(regressor=model_pipeline, transformer=preprocessor_Y)\n",
    "\n",
    "def WA(a, axis, weight):\n",
    "    # Adapted from function_base.py\n",
    "    a = np.asanyarray(a)\n",
    "    wgt = np.asanyarray(weight)\n",
    "    wgt = np.broadcast_to(wgt, (a.ndim-1)*(1,) + wgt.shape)\n",
    "    wgt = wgt.swapaxes(-1, axis)\n",
    "    n = len(a)\n",
    "    avg = np.multiply(a, wgt).sum(axis)/n\n",
    "\n",
    "    return avg\n",
    "\n",
    "def WMAE(y_true, y_pred, sample_weight):\n",
    "    # Adapted from regrssion.py\n",
    "    output_errors = WA(np.abs(y_pred - y_true), weight=sample_weight, axis=0)\n",
    "    avg = np.average(output_errors)\n",
    "\n",
    "    return avg\n",
    "\n",
    "if GRIDSEARCH:\n",
    "    # Define grid parameters to search\n",
    "    grid_params = {\n",
    "        'regressor__multioutreg__estimator__C': [0.0005, 0.001, 0.0015, 0.002]\n",
    "    }\n",
    "\n",
    "    # Define CV by grouping on 'Feature_7'\n",
    "    # See: https://stats.stackexchange.com/questions/95797/how-to-split-the-dataset-for-cross-validation-learning-curve-and-final-evaluat\n",
    "    #      http://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf\n",
    "    group = train_X_df['Feature_7'].values\n",
    "    cv = list(GroupKFold(n_splits=5).split(train_X_df, train_Y_df, group))\n",
    "\n",
    "    # Define grid search scoring metric\n",
    "    scoring = 'neg_mean_absolute_error'\n",
    "\n",
    "    # Define grid search specified scoring and cross-validation generator\n",
    "    print('Running grid search CV...')\n",
    "    gd_sr = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=grid_params,\n",
    "                         scoring=scoring,\n",
    "                         cv=cv,\n",
    "                         # n_jobs=8,\n",
    "                         refit=True)\n",
    "\n",
    "    # Apply grid search and get parameters for best result\n",
    "    gd_sr.fit(train_X_df, train_Y_df)\n",
    "    best_params = gd_sr.best_params_\n",
    "    best_estimator = gd_sr.best_estimator_\n",
    "    score = -gd_sr.best_score_\n",
    "\n",
    "    print(f'Best parameters = {gd_sr.best_params_}')\n",
    "    print(f'Best MAE = {score}')\n",
    "\n",
    "else:\n",
    "    estimator.fit(train_X_df, train_Y_df)\n",
    "    best_estimator = estimator\n",
    "\n",
    "print('Done building model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Model Evaluation** <a class=\"anchor\" id=\"model_evaluation\"></a>\n",
    "\n",
    " We evaluate the model by calculating the Weighted Mean Absolute Error of the model prediction and compare this to the\n",
    " baseline model of just predicting the returns as the mean of the returns in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WMAE score: LOWER is BETTER;)\n",
      "WMAE of fitted model: 27774.420883983385\n",
      "WMAE of baseline model: 27837.43692154106\n"
     ]
    }
   ],
   "source": [
    "# Predict on train and validation data\n",
    "pred_train_Y = best_estimator.predict(train_X_df)\n",
    "\n",
    "# Evaluate predictions on train and validation data and compare with baseline mean prediction\n",
    "mean_Y = [0, 0]\n",
    "mean_Y[0] = train_data[targets[0]].mean()\n",
    "mean_Y[1] = train_data[targets[1]].mean()\n",
    "\n",
    "train_mae = WMAE(train_Y_df, pred_train_Y, sample_weight=train_weights_daily_df)\n",
    "mean_Y_np = np.concatenate((np.full((train_Y_df.shape[0], 1), mean_Y[0]), np.full(\n",
    "    (train_Y_df.shape[0], 1), mean_Y[1])), axis=1)\n",
    "mean_mae = WMAE(train_Y_df, mean_Y_np, sample_weight=train_weights_daily_df)\n",
    "\n",
    "# Print scores\n",
    "print('WMAE score: LOWER is BETTER;)')\n",
    "print(f'WMAE of fitted model: {train_mae}')\n",
    "print(f'WMAE of baseline model: {mean_mae}')\n",
    "\n",
    "# Predict on test data\n",
    "pred_test_Y = best_estimator.predict(test_X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
